{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Analysis of HMM CLassification Surrogate Pipeline\n",
    "**Objective:**  \n",
    "1. Load pre-computed black-box observation sequences.\n",
    "2. Train the full `HMMRegressionSurrogate` pipeline: \n",
    "   a. Fit the HMM component. \n",
    "   b. Extract features using HMM outputs. \n",
    "   c. Fit Regression models (for next-step probability and prediction intervals). \n",
    "3. Analyze the HMM component (states, transitions) - similar to the basic HMM notebook. \n",
    "\n",
    "4. Visualize predictions on test reviews, showing: \n",
    "   a. True black-box probabilities. \n",
    "   b. Decoded HMM states. \n",
    "   c. Regression model's prediction for P(TARGET_SENTIMENT) at the next step. \n",
    "   d. Prediction intervals from the regression model.\"\n",
    "\n",
    "**Alternatively:**\n",
    "1. Load pre-computed black-box observation sequences.\n",
    "2. Train the full Classification pipeline (last 3 cells): \n",
    "   a. Fit the HMM component. \n",
    "   b. Extract features using HMM outputs. \n",
    "   c. Fit Classification models (for final sentiment prediction). \n",
    "3. Visualize predictions on test reviews, showing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "import os  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import pickle  \n",
    "  \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)  \n",
    "  \n",
    "from src.config import (  \n",
    "    LOG_FILE_PATH, TARGET_SENTIMENT,   \n",
    "    NUM_TEST_SAMPLES, MAX_TOKENS, MODEL_NAME, DEVICE,   \n",
    "    NUM_HMM_STATES, HMM_N_ITER, HMM_TOL, HMM_COV_TYPE,   \n",
    "    PROB_THRESHOLDS  \n",
    ")  \n",
    "from src.data_utils import load_imdb_data, preprocess_data_for_inference_logging  \n",
    "from src.black_box_model import BlackBoxSentimentClassifier   , log_inference_trajectories\n",
    "from src.hmm_classification_surrogate import HMMRegressionSurrogate  \n",
    "from src.visualization_utils import plot_hmm_transition_matrix, plot_avg_probabilities_per_state \n",
    "  \n",
    "%matplotlib inline  \n",
    "  \n",
    "PIPELINE_MODEL_SAVE_PREFIX = \"models/hmm_reg_pipeline_imdb_noobs\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  \n",
    "    loaded_data = np.load(LOG_FILE_PATH)  \n",
    "    all_observation_sequences = [loaded_data[f'arr_{i}'] for i in range(len(loaded_data.files))]  \n",
    "    print(f\"Loaded {len(all_observation_sequences)} total observation trajectories from {LOG_FILE_PATH}.\")  \n",
    "    if all_observation_sequences:  \n",
    "        print(f\"Example trajectory 0 shape: {all_observation_sequences[0].shape}\")  \n",
    "except FileNotFoundError:  \n",
    "    print(f\"Error: Log file {LOG_FILE_PATH} not found. Please run Notebook 01_Inference_Logging.ipynb first.\")  \n",
    "    all_observation_sequences = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train HMM Regression Surrogate Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_surrogate = None  \n",
    "TRAIN_PIPELINE = True \n",
    "  \n",
    "if TRAIN_PIPELINE:  \n",
    "    if all_observation_sequences and len(all_observation_sequences) > 50: \n",
    "        pipeline_surrogate = HMMRegressionSurrogate(  \n",
    "            n_states=NUM_HMM_STATES,   \n",
    "            n_iter=HMM_N_ITER, \n",
    "            tol=HMM_TOL,  \n",
    "            covariance_type=HMM_COV_TYPE, \n",
    "            random_state=2,  \n",
    "            regression_method='mlp',   \n",
    "            min_hmm_covar=1e-3,   \n",
    "            \n",
    "            use_feature_scaling=True,  \n",
    "            use_pca_features=False, \n",
    "            n_pca_components=3 \n",
    "        )  \n",
    "          \n",
    "        # The train_full_pipeline method handles HMM training, feature extraction,   \n",
    "        # regression training, and internal validation splits for regression.  \n",
    "        # It uses a split of `all_observation_sequences` for HMM training vs. regression data generation.  \n",
    "        pipeline_surrogate.train_full_pipeline(  \n",
    "            all_observation_sequences,   \n",
    "            validation_split_ratio=0.25, # 25% of total for regression data generation + reg validation  \n",
    "            regression_train_split_ratio=0.8 # Within the 25%, 80% for reg training, 20% for reg validation  \n",
    "        )  \n",
    "  \n",
    "        pipeline_surrogate.save_pipeline(PIPELINE_MODEL_SAVE_PREFIX)  \n",
    "    else:  \n",
    "        print(\"Skipping pipeline training: not enough observation sequences loaded or TRAIN_PIPELINE is False.\")  \n",
    "else:  \n",
    "    print(f\"Attempting to load pre-trained pipeline from prefix {PIPELINE_MODEL_SAVE_PREFIX}\")  \n",
    "    try:  \n",
    "        pipeline_surrogate = HMMRegressionSurrogate.load_pipeline(PIPELINE_MODEL_SAVE_PREFIX)  \n",
    "        if not pipeline_surrogate.is_hmm_trained or not pipeline_surrogate.is_regression_trained:  \n",
    "            print(\"Loaded pipeline is not fully trained. Check saved model.\")  \n",
    "            pipeline_surrogate = None  \n",
    "    except FileNotFoundError:  \n",
    "        print(f\"Error: Pre-trained pipeline not found at {PIPELINE_MODEL_SAVE_PREFIX}.pkl. Please train one first.\")  \n",
    "        pipeline_surrogate = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding HMM states and predicting sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_state_analysis_results_pipeline = None  \n",
    "if pipeline_surrogate and pipeline_surrogate.is_hmm_trained:  \n",
    "    print(\"--- Analyzing HMM Component from Pipeline ---\")  \n",
    "    \n",
    "  \n",
    "    analysis_sequences = all_observation_sequences[: len(all_observation_sequences)] \n",
    "      \n",
    "    decoded_hmm_states_for_analysis = [  \n",
    "        pipeline_surrogate.decode_hmm_sequence(traj)   \n",
    "        for traj in analysis_sequences if traj.shape[0] > 0  \n",
    "    ]  \n",
    "      \n",
    "    valid_obs_for_analysis = [  \n",
    "        traj for traj, states in zip(analysis_sequences, decoded_hmm_states_for_analysis) if states.size > 0  \n",
    "    ]  \n",
    "    valid_decoded_states_for_analysis = [states for states in decoded_hmm_states_for_analysis if states.size > 0]  \n",
    "  \n",
    "    if valid_obs_for_analysis and valid_decoded_states_for_analysis:  \n",
    "        hmm_state_analysis_results_pipeline = pipeline_surrogate.analyze_hmm_states(  \n",
    "            valid_obs_for_analysis,   \n",
    "            valid_decoded_states_for_analysis,   \n",
    "            target_class_idx=TARGET_SENTIMENT  \n",
    "        )  \n",
    "      \n",
    "        if hasattr(pipeline_surrogate, 'hmm_model'):   \n",
    "             plot_hmm_transition_matrix(pipeline_surrogate.hmm_model, state_names=hmm_state_analysis_results_pipeline.get('state_names'))  \n",
    "             plot_avg_probabilities_per_state(hmm_state_analysis_results_pipeline, target_class_idx=TARGET_SENTIMENT)  \n",
    "    else:  \n",
    "        print(\"Could not perform HMM state analysis due to lack of valid data for analysis\")  \n",
    "else:  \n",
    "    print(\"Pipeline or HMM component not trained/loaded. Skipping HMM state analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an MLP regressor on the HMM derived features to predict the next step sentiment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline_surrogate and pipeline_surrogate.is_hmm_trained and pipeline_surrogate.is_regression_trained and hmm_state_analysis_results_pipeline:  \n",
    "    print(\"--- Visualizing Pipeline Predictions on Test Reviews ---\")  \n",
    "      \n",
    "    bb_model_vis = BlackBoxSentimentClassifier(model_name=MODEL_NAME, device=DEVICE)  \n",
    "    tokenizer_vis = bb_model_vis.tokenizer  \n",
    "  \n",
    "    imdb_test_raw_vis = load_imdb_data(split='test', num_samples=NUM_TEST_SAMPLES, shuffle=True)  \n",
    "    processed_test_data_vis = preprocess_data_for_inference_logging(imdb_test_raw_vis, tokenizer_vis)  \n",
    "    test_trajectories_bb_truth = log_inference_trajectories(processed_test_data_vis, bb_model_vis, max_len=MAX_TOKENS)  \n",
    "      \n",
    "    test_tokens_list_vis = []  \n",
    "    for item in processed_test_data_vis:  \n",
    "        cls_token_id = tokenizer_vis.cls_token_id  \n",
    "        actual_tokens = item['tokens']  \n",
    "        if cls_token_id is not None and item['input_ids'][0] == cls_token_id: \n",
    "            pass  \n",
    "        elif cls_token_id is not None: \n",
    "            actual_tokens = [tokenizer_vis.cls_token] + item['tokens']  \n",
    "        test_tokens_list_vis.append(actual_tokens[:MAX_TOKENS])  \n",
    "  \n",
    "    from src.visualization_utils import plot_state_timeline_from_surrogate\n",
    "    \n",
    "    for i, (true_bb_prob_traj, tokens_for_plot) in enumerate(zip(test_trajectories_bb_truth, test_tokens_list_vis)):  \n",
    "        if true_bb_prob_traj.shape[0] < 1: \n",
    "            continue  \n",
    "  \n",
    "        print(f\"\\nVisualizing Test Sample {i+1} (length {len(tokens_for_plot)}):\")  \n",
    "        print(f\"Text: {processed_test_data_vis[i]['text'][:150]}...\")  \n",
    "        \n",
    "        \n",
    "        current_tokens = tokens_for_plot[:len(true_bb_prob_traj)]\n",
    "\n",
    "        try:\n",
    "            # Call the wrapper function\n",
    "            plot_state_timeline_from_surrogate(\n",
    "                hmm_surrogate=pipeline_surrogate,\n",
    "                tokens=current_tokens,           \n",
    "                prob_trajectory=true_bb_prob_traj, \n",
    "                target_class_idx=TARGET_SENTIMENT, \n",
    "                show_predictions=True            \n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"  Error plotting sample {i+1}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() \n",
    "            continue\n",
    "        \n",
    "        if i >= 14: \n",
    "            print(f\"\\nShowing first {i+1} samples. Set this limit higher to see more.\")\n",
    "            break\n",
    "            \n",
    "else:  \n",
    "    print(\"Pipeline components not fully trained/loaded or HMM state analysis missing. Skipping final test visualizations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MLP for early sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "if pipeline_surrogate and pipeline_surrogate.is_hmm_trained:\n",
    "   \n",
    "    bb_model_vis = BlackBoxSentimentClassifier(model_name=MODEL_NAME, device=DEVICE)  \n",
    "    tokenizer_vis = bb_model_vis.tokenizer  \n",
    "   \n",
    "    unbalanced_test_data= load_dataset(\"imdb\", split='test')\n",
    "    unbalanced_test_data = unbalanced_test_data.shuffle(seed = 3)\n",
    "    \n",
    "    \n",
    "   \n",
    "    unbalanced_test_data = unbalanced_test_data.filter(lambda x: len(x['text']) > 200*6)\n",
    "    \n",
    "    \n",
    "    \n",
    "    positive_samples = []\n",
    "    negative_samples = []\n",
    "    num_samples_per_class = 250\n",
    "\n",
    "    for sample in unbalanced_test_data:\n",
    "        \n",
    "        if sample['label'] == 1 and len(positive_samples) < num_samples_per_class:\n",
    "            positive_samples.append(sample)\n",
    "        elif sample['label'] == 0 and len(negative_samples) < num_samples_per_class:\n",
    "            negative_samples.append(sample)\n",
    "        \n",
    "        if len(positive_samples) == num_samples_per_class and len(negative_samples) == num_samples_per_class:\n",
    "            break\n",
    "            \n",
    "    if len(positive_samples) < num_samples_per_class or len(negative_samples) < num_samples_per_class:\n",
    "        raise ValueError(f\"Could not find enough samples for a balanced test set. \"\n",
    "                         f\"Found {len(positive_samples)} positive and {len(negative_samples)} negative samples.\")\n",
    "\n",
    "    imdb_test_raw_vis = positive_samples + negative_samples\n",
    "    import random\n",
    "    random.seed(2)\n",
    "    random.shuffle(imdb_test_raw_vis)\n",
    "    \n",
    "    print(f\" Successfully created a balanced test set with {len(positive_samples)} positive and {len(negative_samples)} negative reviews.\")\n",
    "   \n",
    "    \n",
    "    processed_test_data_vis = preprocess_data_for_inference_logging(imdb_test_raw_vis, tokenizer_vis)  \n",
    "    test_trajectories_bb_truth = log_inference_trajectories(processed_test_data_vis, bb_model_vis, max_len=MAX_TOKENS)\n",
    "   \n",
    "    \n",
    "    final_positive_count = sum(1 for traj in test_trajectories_bb_truth if traj[-1, TARGET_SENTIMENT] >= 0.5)\n",
    "    final_negative_count = len(test_trajectories_bb_truth) - final_positive_count\n",
    "    print(f\" Verified balance: {final_positive_count} positive, {final_negative_count} negative based on transformer final predictions\")\n",
    "    \n",
    "    \n",
    "    K_PREFIX_VALUES = [1,5,10,25,50,75,100,150,175,200]  \n",
    "    BEST_K_PREFIX = 25 \n",
    "\n",
    "    \n",
    "    prefix_results = {}\n",
    "\n",
    "    print(f\"\\n Testing different prefix lengths: {K_PREFIX_VALUES}\")\n",
    "    print(f\" Training on {len(all_observation_sequences)} observation sequences\")\n",
    "    print(f\" Testing on {len(test_trajectories_bb_truth)} test trajectories\")\n",
    "\n",
    "    for k_prefix in K_PREFIX_VALUES:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TESTING PREFIX LENGTH: {k_prefix}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "    \n",
    "        temp_success = pipeline_surrogate.train_final_sentiment_classifier(\n",
    "            all_observation_sequences, \n",
    "            k_prefix=k_prefix\n",
    "        )\n",
    "        \n",
    "        if temp_success:\n",
    "            print(f\" Successfully trained classifier with k_prefix={k_prefix}\")\n",
    "          \n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            prediction_confidence_scores = []\n",
    "            skipped_sequences = 0\n",
    "       \n",
    "            for seq in test_trajectories_bb_truth:\n",
    "                if seq.shape[0] >= k_prefix:\n",
    "                    try:\n",
    "                        prefix = seq[:k_prefix]\n",
    "                        \n",
    "                        true_final_prob = seq[-1, pipeline_surrogate.target_sentiment_idx]\n",
    "                        true_label = 1 if true_final_prob >= 0.5 else 0\n",
    "                        \n",
    "                       \n",
    "                        pred_label, pred_proba = pipeline_surrogate.predict_final_sentiment_from_prefix(prefix)\n",
    "                        \n",
    "                        if pred_label == true_label:\n",
    "                            correct_predictions += 1\n",
    "                        \n",
    "                       \n",
    "                        confidence = pred_proba[pred_label]   #probability of predicted class\n",
    "                        prediction_confidence_scores.append(confidence)\n",
    "                        \n",
    "                        total_predictions += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                else:\n",
    "                    skipped_sequences += 1\n",
    "            \n",
    "            if total_predictions > 0:\n",
    "                accuracy = correct_predictions / total_predictions\n",
    "                avg_confidence = np.mean(prediction_confidence_scores)\n",
    "                \n",
    "                prefix_results[k_prefix] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'avg_confidence': avg_confidence,\n",
    "                    'total_predictions': total_predictions,\n",
    "                    'skipped_sequences': skipped_sequences\n",
    "                }\n",
    "                \n",
    "                print(f\" Test Set Evaluation Results:\")\n",
    "                print(f\"   • Accuracy: {accuracy:.3f} ({correct_predictions}/{total_predictions})\")\n",
    "                print(f\"   • Average Confidence: {avg_confidence:.3f}\")\n",
    "                print(f\"   • Skipped {skipped_sequences} sequences that were shorter than {k_prefix} tokens\")\n",
    "                \n",
    "                if k_prefix == K_PREFIX_VALUES[0] or accuracy > prefix_results[BEST_K_PREFIX]['accuracy']:\n",
    "                    BEST_K_PREFIX = k_prefix\n",
    "                    print(f\" New best prefix length: {BEST_K_PREFIX}\")\n",
    "            else:\n",
    "                print(f\" No valid predictions made with k_prefix={k_prefix}\")\n",
    "        else:\n",
    "            print(f\" Failed to train classifier with k_prefix={k_prefix}\")\n",
    "\n",
    " \n",
    "\n",
    "    final_success = pipeline_surrogate.train_final_sentiment_classifier(\n",
    "        all_observation_sequences, \n",
    "        k_prefix=BEST_K_PREFIX\n",
    "    )\n",
    "\n",
    "    if final_success:\n",
    "        print(f\"Final sentiment classifier trained successfully!\")\n",
    "        \n",
    "       \n",
    "        \n",
    "       \n",
    "        \n",
    "     \n",
    "        print(f\"\\n PREFIX LENGTH COMPARISON (TEST SET PERFORMANCE):\")\n",
    "        print(f\"{'Prefix Length':<15} {'Accuracy':<10} {'Confidence':<12} {'Predictions':<12} {'Skipped':<12}\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        for k, results in prefix_results.items():\n",
    "            print(f\"{k:<15} {results['accuracy']:<10.3f} {results['avg_confidence']:<12.3f} {results['total_predictions']:<12} {results['skipped_sequences']:<12}\")\n",
    "        \n",
    "        print(f\"\\n EXAMPLE TEST PREDICTIONS WITH PREFIX LENGTH {BEST_K_PREFIX}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        example_count = 0\n",
    "        positive_examples = 0\n",
    "        negative_examples = 0\n",
    "        \n",
    "        for i, seq in enumerate(test_trajectories_bb_truth[:20]):  \n",
    "            if seq.shape[0] >= BEST_K_PREFIX and example_count < 8:\n",
    "                try:\n",
    "                   \n",
    "                    prefix = seq[:BEST_K_PREFIX]\n",
    "                    true_final_prob = seq[-1, pipeline_surrogate.target_sentiment_idx]\n",
    "                    true_label = 1 if true_final_prob >= 0.5 else 0\n",
    "                    \n",
    "                    \n",
    "                    pred_label, pred_proba = pipeline_surrogate.predict_final_sentiment_from_prefix(prefix)\n",
    "                    \n",
    "                 \n",
    "                    if (true_label == 1 and positive_examples < 4) or (true_label == 0 and negative_examples < 4):\n",
    "                        print(f\"\\nTest Example {example_count + 1} (Sequence {i}):\")\n",
    "                        print(f\"   Sequence Length: {seq.shape[0]} tokens\")\n",
    "                        print(f\"   Review Text: {processed_test_data_vis[i]['text'][:100]}...\")\n",
    "                        print(f\"   True Final Sentiment: {'POSITIVE' if true_label == 1 else 'NEGATIVE'} (prob={true_final_prob:.3f})\")\n",
    "                        print(f\"   Predicted from Prefix: {'POSITIVE' if pred_label == 1 else 'NEGATIVE'}\")\n",
    "                        print(f\"   Prediction Probabilities: [Neg: {pred_proba[0]:.3f}, Pos: {pred_proba[1]:.3f}]\")\n",
    "                        print(f\"   Correct: {'YES' if pred_label == true_label else 'NO'}\")\n",
    "                        \n",
    "                        prefix_target_probs = prefix[:, pipeline_surrogate.target_sentiment_idx]\n",
    "                        print(f\"   Prefix Target Sentiment Evolution: {[f'{p:.3f}' for p in prefix_target_probs]}\")\n",
    "                        \n",
    "                        example_count += 1\n",
    "                        if true_label == 1:\n",
    "                            positive_examples += 1\n",
    "                        else:\n",
    "                            negative_examples += 1\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        all_true_labels = []\n",
    "        all_pred_labels = []\n",
    "        all_pred_probas = []\n",
    "        sequence_lengths = []\n",
    "        confidence_by_correctness = {'correct': [], 'incorrect': []}\n",
    "        \n",
    "        for seq in test_trajectories_bb_truth:\n",
    "            if seq.shape[0] >= BEST_K_PREFIX:\n",
    "                try:\n",
    "                    prefix = seq[:BEST_K_PREFIX]\n",
    "                    true_final_prob = seq[-1, pipeline_surrogate.target_sentiment_idx]\n",
    "                    true_label = 1 if true_final_prob >= 0.5 else 0\n",
    "                    \n",
    "                    pred_label, pred_proba = pipeline_surrogate.predict_final_sentiment_from_prefix(prefix)\n",
    "                    \n",
    "                    all_true_labels.append(true_label)\n",
    "                    all_pred_labels.append(pred_label)\n",
    "                    all_pred_probas.append(pred_proba[1])  # Probability of positive class\n",
    "                    sequence_lengths.append(seq.shape[0])\n",
    "                    \n",
    "            \n",
    "                    confidence = pred_proba[pred_label]\n",
    "                    if pred_label == true_label:\n",
    "                        confidence_by_correctness['correct'].append(confidence)\n",
    "                    else:\n",
    "                        confidence_by_correctness['incorrect'].append(confidence)\n",
    "                        \n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        if all_true_labels:\n",
    "            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "            \n",
    "            accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "            precision = precision_score(all_true_labels, all_pred_labels, zero_division=0)\n",
    "            recall = recall_score(all_true_labels, all_pred_labels, zero_division=0)\n",
    "            f1 = f1_score(all_true_labels, all_pred_labels, zero_division=0)\n",
    "            \n",
    "            print(f\" Test Set Classification Metrics (n={len(all_true_labels)}):\")\n",
    "            print(f\"   • Accuracy:  {accuracy:.3f}\")\n",
    "            print(f\"   • Precision: {precision:.3f}\")\n",
    "            print(f\"   • Recall:    {recall:.3f}\")\n",
    "            print(f\"   • F1-Score:  {f1:.3f}\")\n",
    "     \n",
    "            cm = confusion_matrix(all_true_labels, all_pred_labels)\n",
    "            print(f\"\\n Confusion Matrix:\")\n",
    "            print(f\"   Predicted:  Neg  Pos\")\n",
    "            print(f\"   True Neg:   {cm[0,0]:<4} {cm[0,1]:<4}\")\n",
    "            print(f\"   True Pos:   {cm[1,0]:<4} {cm[1,1]:<4}\")\n",
    "            \n",
    "            \n",
    "            if confidence_by_correctness['correct'] and confidence_by_correctness['incorrect']:\n",
    "                avg_conf_correct = np.mean(confidence_by_correctness['correct'])\n",
    "                avg_conf_incorrect = np.mean(confidence_by_correctness['incorrect'])\n",
    "                \n",
    "                print(f\"\\n Confidence Analysis:\")\n",
    "                print(f\"   • Avg confidence (correct predictions):   {avg_conf_correct:.3f}\")\n",
    "                print(f\"   • Avg confidence (incorrect predictions): {avg_conf_incorrect:.3f}\")\n",
    "                print(f\"   • Confidence gap: {avg_conf_correct - avg_conf_incorrect:.3f}\")\n",
    "            \n",
    "            \n",
    "            avg_seq_length = np.mean(sequence_lengths)\n",
    "            print(f\"\\n Sequence Length Analysis:\")\n",
    "            print(f\"   • Average test sequence length: {avg_seq_length:.1f} tokens\")\n",
    "            print(f\"   • Prefix covers: {(BEST_K_PREFIX/avg_seq_length)*100:.1f}% of average sequence\")\n",
    "            \n",
    "       \n",
    "            positive_ratio = np.mean(all_true_labels)\n",
    "            print(f\"\\n Test Set Class Distribution:\")\n",
    "            print(f\"   • Positive samples: {np.sum(all_true_labels)}/{len(all_true_labels)} ({positive_ratio:.1%})\")\n",
    "            print(f\"   • Negative samples: {len(all_true_labels) - np.sum(all_true_labels)}/{len(all_true_labels)} ({1-positive_ratio:.1%})\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\" FINAL SENTIMENT CLASSIFICATION ANALYSIS COMPLETE\")\n",
    "        print(f\" Best prefix length: {BEST_K_PREFIX} tokens\")\n",
    "        print(f\" Evaluated on {len(test_trajectories_bb_truth)} test reviews\")\n",
    "        print(f\" Ready for early sentiment prediction!\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    else:\n",
    "        print(\" Failed to train final sentiment classifier\")\n",
    "        \n",
    "else:\n",
    "    print(\" Pipeline surrogate not available or HMM not trained. Skipping final sentiment classification analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Accuracy vs k initial tokens and the confusion matrix for the best k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if prefix_results:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    k_values = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for k_prefix in sorted(prefix_results.keys()):\n",
    "        k_values.append(k_prefix)\n",
    "        accuracies.append(prefix_results[k_prefix]['accuracy'])\n",
    "    \n",
    "    final_positive_count = sum(1 for traj in test_trajectories_bb_truth if traj[-1, TARGET_SENTIMENT] >= 0.5)\n",
    "    class_balance = final_positive_count / len(test_trajectories_bb_truth)\n",
    "    mean_token_length = np.mean([traj.shape[0] for traj in test_trajectories_bb_truth])\n",
    "    baseline_acc = max(class_balance, 1-class_balance)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    ax1.plot(k_values, accuracies, 'b-o', linewidth=3, markersize=8, \n",
    "             markerfacecolor='lightblue', markeredgecolor='darkblue', markeredgewidth=2)\n",
    "    \n",
    "    best_accuracy = max(accuracies)\n",
    "    best_k = k_values[accuracies.index(best_accuracy)]\n",
    "    ax1.plot(best_k, best_accuracy, 'ro', markersize=12, \n",
    "             label=f'Best: k={best_k}, acc={best_accuracy:.3f}')\n",
    "    \n",
    "    ax1.set_xlabel('Initial Prefix Length (tokens)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Classification Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Accuracy vs Initial Prefix Length', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for i, (k, acc) in enumerate(zip(k_values, accuracies)):\n",
    "        ax1.annotate(f'{acc:.3f}', (k, acc), textcoords=\"offset points\", \n",
    "                    xytext=(0,15), ha='center', fontsize=10, fontweight='bold',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.7))\n",
    "    \n",
    "    info_text = f\"\"\"Test Set Info:\n",
    "• Total Reviews: {len(test_trajectories_bb_truth)}\n",
    "• Class Balance: {class_balance:.1%} pos, {1-class_balance:.1%} neg\n",
    "• Mean Length: {mean_token_length:.1f} tokens\n",
    "• Baseline: {baseline_acc:.3f}\"\"\"\n",
    "    \n",
    "    ax1.text(0.02, 0.02, info_text, transform=ax1.transAxes, \n",
    "             fontsize=10, verticalalignment='bottom',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.4\", facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    ax1.axhline(y=baseline_acc, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Baseline: {baseline_acc:.3f}')\n",
    "    \n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.legend(loc='lower right', fontsize=10)\n",
    "    \n",
    "\n",
    "    if 'all_true_labels' in locals() and 'all_pred_labels' in locals():\n",
    "        cm = confusion_matrix(all_true_labels, all_pred_labels)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['Negative', 'Positive'], \n",
    "                   yticklabels=['Negative', 'Positive'],\n",
    "                   ax=ax2, cbar_kws={'label': 'Count'})\n",
    "        \n",
    "        ax2.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title(f'Confusion Matrix (k={BEST_K_PREFIX} tokens)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "\n",
    "        \n",
    "        ax2.text(0.5, -0.18, f\"Precision: {precision:.3f}  Recall: {recall:.3f}  F1-Score: {f1:.3f}\", \n",
    "                transform=ax2.transAxes, fontsize=10, verticalalignment='center', ha='center',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.4\", facecolor='lightyellow', alpha=0.8))\n",
    "    else:\n",
    "        print(\"Generating confusion matrix data for best prefix length...\")\n",
    "        temp_true_labels = []\n",
    "        temp_pred_labels = []\n",
    "        \n",
    "        for seq in test_trajectories_bb_truth:\n",
    "            if seq.shape[0] >= best_k:\n",
    "                try:\n",
    "                    prefix = seq[:best_k]\n",
    "                    true_final_prob = seq[-1, pipeline_surrogate.target_sentiment_idx]\n",
    "                    true_label = 1 if true_final_prob >= 0.5 else 0\n",
    "                    \n",
    "                    pred_label, pred_proba = pipeline_surrogate.predict_final_sentiment_from_prefix(prefix)\n",
    "                    \n",
    "                    temp_true_labels.append(true_label)\n",
    "                    temp_pred_labels.append(pred_label)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        if temp_true_labels:\n",
    "            cm = confusion_matrix(temp_true_labels, temp_pred_labels)\n",
    "            \n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                       xticklabels=['Negative', 'Positive'], \n",
    "                       yticklabels=['Negative', 'Positive'],\n",
    "                       ax=ax2, cbar_kws={'label': 'Count'})\n",
    "            \n",
    "            ax2.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "            ax2.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "            ax2.set_title(f'Confusion Matrix (k={best_k} tokens)', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            metrics_text = f\"\"\"Performance Metrics:\n",
    "- Precision: {precision:.3f}\n",
    "- Recall: {recall:.3f}\n",
    "- F1-Score: {f1:.3f}\n",
    "\"\"\"\n",
    "            \n",
    "            ax2.text(1.05, 0.5, metrics_text, transform=ax2.transAxes, \n",
    "                    fontsize=10, verticalalignment='center',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.4\", facecolor='lightyellow', alpha=0.8))\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'No confusion matrix data available', \n",
    "                    transform=ax2.transAxes, ha='center', va='center', fontsize=12)\n",
    "            ax2.set_title('Confusion Matrix - No Data', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "   \n",
    "else:\n",
    "    print(\"No prefix results available for plotting. Run the prefix analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare HMM trained MLP with direct trajectory-based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "if 'test_trajectories_bb_truth' in locals() and test_trajectories_bb_truth:\n",
    "    print(\"=\"*80)\n",
    "    print(\"DIRECT TRAJECTORY-BASED CLASSIFICATION ANALYSIS\")\n",
    "    print(\"Training MLP directly on raw probability trajectories (No HMM features)\")\n",
    "    print(\"=\"*80)\n",
    " \n",
    "    K_PREFIX_VALUES_DIRECT = [1,5,10,25,50,75,100,150,175,200]\n",
    "    direct_prefix_results = {}\n",
    "    \n",
    "   \n",
    "    \n",
    "    print(f\"\\nTotal available test trajectories: {len(test_trajectories_bb_truth)}\")\n",
    "    \n",
    "    for k_prefix in K_PREFIX_VALUES_DIRECT:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DIRECT TRAJECTORY CLASSIFICATION - PREFIX LENGTH: {k_prefix}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "   \n",
    "        X_direct = []\n",
    "        y_direct = []\n",
    "        \n",
    "        for seq in test_trajectories_bb_truth:\n",
    "            if seq.shape[0] >= k_prefix:\n",
    "           \n",
    "                prefix_features = seq[:k_prefix].flatten()  \n",
    "              \n",
    "                true_final_prob = seq[-1, TARGET_SENTIMENT]\n",
    "                true_label = 1 if true_final_prob >= 0.5 else 0\n",
    "                \n",
    "                X_direct.append(prefix_features)\n",
    "                y_direct.append(true_label)\n",
    "        \n",
    "        if len(X_direct) < 20:  \n",
    "            print(f\"   Insufficient data for k_prefix={k_prefix} (only {len(X_direct)} samples)\")\n",
    "            continue\n",
    "            \n",
    "        X_direct = np.array(X_direct)\n",
    "        y_direct = np.array(y_direct)\n",
    "        \n",
    "        print(f\"   Extracted {X_direct.shape[0]} samples with {X_direct.shape[1]} features each\")\n",
    "        print(f\"   Feature vector shape per sample: {k_prefix} steps × {seq.shape[1]} prob dimensions = {X_direct.shape[1]} total features\")\n",
    "        \n",
    "        # Check class balance\n",
    "        pos_count = np.sum(y_direct)\n",
    "        neg_count = len(y_direct) - pos_count\n",
    "        print(f\"   Class distribution: {pos_count} positive, {neg_count} negative\")\n",
    "        \n",
    "       \n",
    "        try:\n",
    "            X_train_direct, X_test_direct, y_train_direct, y_test_direct = train_test_split(\n",
    "                X_direct, y_direct, \n",
    "                test_size=0.3, \n",
    "                random_state=2,  \n",
    "                stratify=y_direct if np.unique(y_direct).size > 1 else None\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(f\"   Error in train/test split: {e}\")\n",
    "            continue\n",
    "        \n",
    "     \n",
    "        scaler_direct = StandardScaler()\n",
    "        X_train_scaled = scaler_direct.fit_transform(X_train_direct)\n",
    "        X_test_scaled = scaler_direct.transform(X_test_direct)\n",
    "        \n",
    "        \n",
    "        mlp_direct = MLPClassifier(\n",
    "            hidden_layer_sizes=( 64, 32),  \n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=0.01,\n",
    "            random_state=2,\n",
    "            max_iter=300,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1,\n",
    "            n_iter_no_change=15\n",
    "        )\n",
    "        \n",
    "        print(f\"   Training MLP on {X_train_scaled.shape[0]} training samples...\")\n",
    "        try:\n",
    "            mlp_direct.fit(X_train_scaled, y_train_direct)\n",
    "            \n",
    "            \n",
    "            y_pred_direct = mlp_direct.predict(X_test_scaled)\n",
    "            y_pred_proba_direct = mlp_direct.predict_proba(X_test_scaled)\n",
    "            \n",
    "          \n",
    "            accuracy = accuracy_score(y_test_direct, y_pred_direct)\n",
    "            precision = precision_score(y_test_direct, y_pred_direct, zero_division=0)\n",
    "            recall = recall_score(y_test_direct, y_pred_direct, zero_division=0)\n",
    "            f1 = f1_score(y_test_direct, y_pred_direct, zero_division=0)\n",
    "            \n",
    "      \n",
    "            confidences = [y_pred_proba_direct[i, y_pred_direct[i]] for i in range(len(y_pred_direct))]\n",
    "            avg_confidence = np.mean(confidences)\n",
    "            \n",
    "    \n",
    "            direct_prefix_results[k_prefix] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'avg_confidence': avg_confidence,\n",
    "                'total_test_samples': len(y_test_direct),\n",
    "                'training_samples': len(y_train_direct),\n",
    "                'feature_dimension': X_direct.shape[1]\n",
    "            }\n",
    "            \n",
    "            print(f\"   RESULTS:\")\n",
    "            print(f\"     - Accuracy:    {accuracy:.4f}\")\n",
    "            print(f\"     - Precision:   {precision:.4f}\")\n",
    "            print(f\"     - Recall:      {recall:.4f}\")\n",
    "            print(f\"     - F1-Score:    {f1:.4f}\")\n",
    "            print(f\"     - Avg Confidence: {avg_confidence:.4f}\")\n",
    "            print(f\"     - Test samples: {len(y_test_direct)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Error training MLP: {e}\")\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    if direct_prefix_results:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"DIRECT TRAJECTORY CLASSIFICATION SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"{'Prefix':<8} {'Accuracy':<10} {'Precision':<11} {'Recall':<8} {'F1':<8} {'Confidence':<11} {'Test Samples':<12} {'Features':<10}\")\n",
    "        print(\"-\" * 95)\n",
    "        \n",
    "        best_direct_accuracy = 0\n",
    "        best_direct_k = None\n",
    "        \n",
    "        for k in sorted(direct_prefix_results.keys()):\n",
    "            results = direct_prefix_results[k]\n",
    "            print(f\"{k:<8} {results['accuracy']:<10.4f} {results['precision']:<11.4f} {results['recall']:<8.4f} \"\n",
    "                  f\"{results['f1']:<8.4f} {results['avg_confidence']:<11.4f} {results['total_test_samples']:<12} {results['feature_dimension']:<10}\")\n",
    "            \n",
    "            if results['accuracy'] > best_direct_accuracy:\n",
    "                best_direct_accuracy = results['accuracy']\n",
    "                best_direct_k = k\n",
    "        \n",
    "        print(f\"\\nBest Direct Trajectory Performance: k={best_direct_k}, accuracy={best_direct_accuracy:.4f}\")\n",
    "        \n",
    "    \n",
    "        if 'prefix_results' in locals() and prefix_results:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"COMPARISON: HMM-BASED vs DIRECT TRAJECTORY CLASSIFICATION\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            print(f\"{'Prefix':<8} {'HMM Accuracy':<13} {'Direct Accuracy':<16} {'Difference':<12} {'Winner':<10}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            common_k_values = set(prefix_results.keys()) & set(direct_prefix_results.keys())\n",
    "            hmm_wins = 0\n",
    "            direct_wins = 0\n",
    "            \n",
    "            for k in sorted(common_k_values):\n",
    "                hmm_acc = prefix_results[k]['accuracy']\n",
    "                direct_acc = direct_prefix_results[k]['accuracy']\n",
    "                diff = direct_acc - hmm_acc\n",
    "                winner = \"Direct\" if diff > 0 else \"HMM\" if diff < 0 else \"Tie\"\n",
    "                \n",
    "                if diff > 0:\n",
    "                    direct_wins += 1\n",
    "                elif diff < 0:\n",
    "                    hmm_wins += 1\n",
    "                \n",
    "                print(f\"{k:<8} {hmm_acc:<13.4f} {direct_acc:<16.4f} {diff:<+12.4f} {winner:<10}\")\n",
    "            \n",
    "            print(f\"\\nOverall Comparison:\")\n",
    "            print(f\"  • HMM-based approach wins: {hmm_wins}/{len(common_k_values)} prefix lengths\")\n",
    "            print(f\"  • Direct trajectory approach wins: {direct_wins}/{len(common_k_values)} prefix lengths\")\n",
    "            \n",
    "            # Find best overall performers\n",
    "            best_hmm_k = max(prefix_results.keys(), key=lambda k: prefix_results[k]['accuracy'])\n",
    "            best_hmm_acc = prefix_results[best_hmm_k]['accuracy']\n",
    "            \n",
    "            print(f\"\\nBest Overall Performance:\")\n",
    "            print(f\"  • HMM-based:       k={best_hmm_k}, accuracy={best_hmm_acc:.4f}\")\n",
    "            print(f\"  • Direct trajectory: k={best_direct_k}, accuracy={best_direct_accuracy:.4f}\")\n",
    "            \n",
    "            if best_direct_accuracy > best_hmm_acc:\n",
    "                print(f\"   Winner: Direct trajectory approach (+{best_direct_accuracy - best_hmm_acc:.4f})\")\n",
    "            elif best_hmm_acc > best_direct_accuracy:\n",
    "                print(f\"   Winner: HMM-based approach (+{best_hmm_acc - best_direct_accuracy:.4f})\")\n",
    "            else:\n",
    "                print(f\"   Tie between approaches\")\n",
    "                \n",
    "        if 'prefix_results' in locals() and prefix_results:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "            \n",
    "            # Plot accuracy comparison\n",
    "            common_k_values = sorted(set(prefix_results.keys()) & set(direct_prefix_results.keys()))\n",
    "            hmm_accuracies = [prefix_results[k]['accuracy'] for k in common_k_values]\n",
    "            direct_accuracies = [direct_prefix_results[k]['accuracy'] for k in common_k_values]\n",
    "            \n",
    "            ax1.plot(common_k_values, hmm_accuracies, 'b-o', linewidth=3, markersize=8, \n",
    "                     label='HMM-based Classification', markerfacecolor='lightblue')\n",
    "            ax1.plot(common_k_values, direct_accuracies, 'r-s', linewidth=3, markersize=8, \n",
    "                     label='Direct Trajectory Classification', markerfacecolor='lightcoral')\n",
    "            \n",
    "            ax1.set_xlabel('Prefix Length (tokens)', fontsize=12, fontweight='bold')\n",
    "            ax1.set_ylabel('Classification Accuracy', fontsize=12, fontweight='bold')\n",
    "            ax1.set_title('HMM-based vs Direct Trajectory Classification', fontsize=14, fontweight='bold')\n",
    "            ax1.legend(fontsize=11)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.set_ylim([0, 1])\n",
    "            \n",
    "            direct_feature_dims = [direct_prefix_results[k]['feature_dimension'] for k in common_k_values]\n",
    "            hmm_feature_dims = [pipeline_surrogate.n_states] * len(common_k_values)  # HMM uses state posteriors\n",
    "            \n",
    "            ax2.plot(common_k_values, direct_feature_dims, 'r-s', linewidth=3, markersize=8, \n",
    "                     label='Direct Trajectory Features', markerfacecolor='lightcoral')\n",
    "            ax2.axhline(y=hmm_feature_dims[0], color='blue', linestyle='--', linewidth=3, \n",
    "                       label=f'HMM Features (constant = {hmm_feature_dims[0]})')\n",
    "            \n",
    "            ax2.set_xlabel('Prefix Length (tokens)', fontsize=12, fontweight='bold')\n",
    "            ax2.set_ylabel('Number of Features', fontsize=12, fontweight='bold')\n",
    "            ax2.set_title('Feature Dimensionality Comparison', fontsize=14, fontweight='bold')\n",
    "            ax2.legend(fontsize=11)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.set_yscale('log')  \n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "    else:\n",
    "        print(\"No direct trajectory classification results available.\")\n",
    "        \n",
    "else:\n",
    "    print(\"No test trajectories available for direct classification analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
